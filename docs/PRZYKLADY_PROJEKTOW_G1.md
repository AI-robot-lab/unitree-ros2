# PrzykÅ‚adowe Projekty dla Robota Unitree G1 EDU

## ğŸ“– Spis treÅ›ci
1. [Projekty podstawowe](#projekty-podstawowe)
2. [Projekty Å›rednio-zaawansowane](#projekty-Å›rednio-zaawansowane)
3. [Projekty zaawansowane](#projekty-zaawansowane)
4. [Projekty badawcze](#projekty-badawcze)
5. [WskazÃ³wki realizacji](#wskazÃ³wki-realizacji)

---

## Projekty podstawowe

### Projekt 1: Dashboard MonitorujÄ…cy ğŸ“Š

**Poziom:** ğŸŸ¢ Podstawowy  
**Czas realizacji:** 1-2 tygodnie  
**UmiejÄ™tnoÅ›ci:** Odczyt danych, wizualizacja, interfejsy uÅ¼ytkownika

#### Opis
StwÃ³rz aplikacjÄ™ z interfejsem graficznym, ktÃ³ra w czasie rzeczywistym wyÅ›wietla:
- Stan wszystkich 29 stawÃ³w (pozycje, prÄ™dkoÅ›ci, temperatury)
- Dane z IMU (orientacja, przyspieszenia)
- Stan baterii
- OstrzeÅ¼enia o przekroczeniu limitÃ³w

#### Cele edukacyjne
- âœ… Nauka subskrypcji topikÃ³w ROS2
- âœ… Przetwarzanie strumienia danych
- âœ… Tworzenie GUI (np. Qt, web interface)
- âœ… Wizualizacja danych w czasie rzeczywistym

#### Sugerowane technologie
- **Backend:** ROS2 + Python lub C++
- **Frontend:** 
  - PyQt5/PySide6 (Python GUI)
  - RViz2 (wbudowane narzÄ™dzie ROS2)
  - Web: React + rosbridge
- **Wykresy:** Matplotlib, Plotly

#### Kamienie milowe
1. **TydzieÅ„ 1:**
   - Odczyt danych z topiku `/lowstate`
   - WyÅ›wietlenie danych w konsoli
   - Prosty GUI z podstawowymi informacjami

2. **TydzieÅ„ 2:**
   - Wykresy w czasie rzeczywistym
   - System ostrzeÅ¼eÅ„
   - Zapis danych do pliku

#### Rozszerzenia
- ğŸ“ˆ Historia danych (wykresy 3D pozycji stawÃ³w)
- ğŸ¨ Wizualizacja 3D robota (URDF model)
- ğŸ”” Powiadomienia push o krytycznych stanach
- ğŸ’¾ Eksport danych do CSV/JSON

---

### Projekt 2: Kontroler GestÃ³w ğŸ¤²

**Poziom:** ğŸŸ¢ Podstawowy  
**Czas realizacji:** 2-3 tygodnie  
**UmiejÄ™tnoÅ›ci:** Sterowanie, interpolacja trajektorii

#### Opis
Zaprogramuj robota do wykonywania zestawu prostych gestÃ³w:
- ğŸ‘‹ Machanie rÄ™kÄ… na powitanie
- ğŸ‘ Kciuk w gÃ³rÄ™ (aprobujÄ™)
- ğŸ‘ Kciuk w dÃ³Å‚ (dezaprobata)
- ğŸ¤ Gest podania rÄ™ki
- ğŸ™‹ Podniesienie rÄ™ki

#### Cele edukacyjne
- âœ… Sterowanie ramionami robota
- âœ… Planowanie trajektorii
- âœ… Interpolacja miÄ™dzy pozycjami
- âœ… Tworzenie biblioteki ruchÃ³w

#### Architektura rozwiÄ…zania
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Interfejs       â”‚ â† WybÃ³r gestu przez uÅ¼ytkownika
â”‚  (CLI lub GUI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gesture Library â”‚ â† Definicje gestÃ³w (JSON/YAML)
â”‚  (baza gestÃ³w)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Trajectory       â”‚ â† Generowanie pÅ‚ynnej trajektorii
â”‚ Planner          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Low-Level        â”‚ â† WysyÅ‚anie komend do robota
â”‚ Controller       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### PrzykÅ‚adowa definicja gestu (YAML)
```yaml
wave_hand:
  name: "Machanie rÄ™kÄ…"
  duration: 3.0  # sekundy
  arm: "right"   # lewe/prawe ramiÄ™
  keyframes:
    - time: 0.0
      joints:
        shoulder_pitch: 1.57  # 90Â° (ramiÄ™ w gÃ³rÄ™)
        shoulder_roll: 0.0
        elbow: 0.0
    - time: 1.0
      joints:
        shoulder_pitch: 1.57
        elbow: 1.0  # Zegnij Å‚okieÄ‡
    - time: 2.0
      joints:
        shoulder_pitch: 1.57
        elbow: 0.0  # Wyprostuj
    - time: 3.0
      joints:
        shoulder_pitch: 0.0  # OpuÅ›Ä‡ ramiÄ™
        elbow: 0.0
```

#### Kamienie milowe
1. **TydzieÅ„ 1:**
   - Podstawowe sterowanie ramionami
   - Jeden prosty gest (machanie)
   - Interpolacja liniowa miÄ™dzy punktami

2. **TydzieÅ„ 2:**
   - Biblioteka 5 gestÃ³w
   - Åadowanie gestÃ³w z pliku YAML
   - Interfejs wyboru gestu

3. **TydzieÅ„ 3:**
   - PÅ‚ynniejsza interpolacja (kubiczna)
   - Synchronizacja obu ramion
   - MoÅ¼liwoÅ›Ä‡ nagrywania wÅ‚asnych gestÃ³w

#### Rozszerzenia
- ğŸ® Sterowanie gestami przez kontroler
- ğŸ—£ï¸ Aktywacja gÅ‚osowa (integracja z ROS audio)
- ğŸ“¹ Rozpoznawanie gestÃ³w ludzkich (kamera + CV)
- ğŸ­ Ekspresja caÅ‚ego ciaÅ‚a (+ nogi)

---

### Projekt 3: Teleoperation z Kontrolera ğŸ®

**Poziom:** ğŸŸ¢ Podstawowy  
**Czas realizacji:** 2 tygodnie  
**UmiejÄ™tnoÅ›ci:** Odczyt kontrolera, mapowanie wartoÅ›ci

#### Opis
Zaimplementuj intuicyjne sterowanie robotem uÅ¼ywajÄ…c bezprzewodowego kontrolera Unitree:
- **Lewy joystick:** Kontrola pozycji korpusu (pochylenie)
- **Prawy joystick:** Sterowanie ramieniem
- **Przyciski:** PrzeÅ‚Ä…czanie trybÃ³w, wykonanie gestÃ³w

#### Cele edukacyjne
- âœ… Odczyt danych z kontrolera
- âœ… Mapowanie analogowych sygnaÅ‚Ã³w
- âœ… Implementacja stref martwych
- âœ… Zmiana trybÃ³w sterowania

#### Mapa przypisaÅ„ kontrolera
```
Kontroler Unitree:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [L2]                             [R2]           â”‚
â”‚  [L1]                             [R1]           â”‚
â”‚                                                  â”‚
â”‚    â•”â•â•â•â•—                           (Y)           â”‚
â”‚    â•‘ L â•‘                      (X)  [A]  (B)      â”‚
â”‚    â•šâ•â•â•â•                                         â”‚
â”‚                                   â•”â•â•â•â•—          â”‚
â”‚                                   â•‘ R â•‘          â”‚
â”‚                                   â•šâ•â•â•â•          â”‚
â”‚                                                  â”‚
â”‚  [SELECT]                     [START]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Proponowane mapowanie:
- L joystick: Pochylenie korpusu (pitch/roll)
- R joystick: Sterowanie prawym ramieniem
- L1: PrzeÅ‚Ä…cz na lewe ramiÄ™
- R1: PrzeÅ‚Ä…cz na prawe ramiÄ™
- A: Pozycja zero
- B: Zapisz pozycjÄ™
- X: OdtwÃ³rz pozycjÄ™
- Y: Tryb awaryjny (disable)
- Start: WÅ‚Ä…cz/wyÅ‚Ä…cz sterowanie
```

#### Kamienie milowe
1. **TydzieÅ„ 1:**
   - Odczyt wszystkich przyciskÃ³w i joystickÃ³w
   - Sterowanie jednym ramieniem
   - Strefa martwa i skalowanie

2. **TydzieÅ„ 2:**
   - PrzeÅ‚Ä…czanie trybÃ³w
   - Sterowanie pozycjÄ… korpusu
   - Zabezpieczenia i limity

#### Rozszerzenia
- ğŸ® Wsparcie dla innych kontrolerÃ³w (Xbox, PS4)
- ğŸ“ Konfigurowalny mapping (plik konfiguracyjny)
- ğŸšï¸ Regulowana czuÅ‚oÅ›Ä‡ joystickÃ³w
- ğŸ”Š Feedback dÅºwiÄ™kowy/haptyczny

---

## Projekty Å›rednio-zaawansowane

### Projekt 4: Object Pointing System ğŸ¯

**Poziom:** ğŸŸ¡ Åšrednio-zaawansowany  
**Czas realizacji:** 3-4 tygodnie  
**UmiejÄ™tnoÅ›ci:** Kinematyka, transformacje, percepcja

#### Opis
Robot wykrywa obiekty w przestrzeni 3D i wskazuje je palcem/ramieniem. System potrafi:
- WykryÄ‡ pozycjÄ™ obiektu (kamera RGB-D lub LiDAR)
- ObliczyÄ‡ kinematykÄ™ odwrotnÄ… ramienia
- WskazaÄ‡ obiekt palcem z precyzjÄ…

#### Cele edukacyjne
- âœ… Transformacje przestrzenne (TF2)
- âœ… Kinematyka odwrotna (Inverse Kinematics)
- âœ… Integracja z sensorami
- âœ… Koordynacja oko-rÄ™ka

#### Architektura systemu
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kamera    â”‚ â† Wykrywa obiekt (3D point cloud)
â”‚   lub LiDAR â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Pozycja obiektu w ukÅ‚adzie kamery
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TF2 Trans-  â”‚ â† Transformacja do ukÅ‚adu robota
â”‚ formation   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Pozycja obiektu w ukÅ‚adzie bazowym
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inverse    â”‚ â† Obliczenie kÄ…tÃ³w stawÃ³w
â”‚ Kinematics  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ KÄ…ty stawÃ³w [Î¸1, Î¸2, ...]
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Motion    â”‚ â† PÅ‚ynna trajektoria
â”‚  Planning   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    G1       â”‚ â† Wykonanie ruchu
â”‚   Robot     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Etapy realizacji

**Faza 1: Symulacja (tydzieÅ„ 1-2)**
- Wirtualne obiekty w RViz
- Kinematyka odwrotna dla ramienia G1
- Testy z rÃ³Å¼nymi pozycjami

**Faza 2: Percepcja (tydzieÅ„ 2-3)**
- Integracja z kamerÄ…/LiDARem
- Detekcja obiektÃ³w (np. ArUco markers)
- Transformacje TF2

**Faza 3: Integracja (tydzieÅ„ 3-4)**
- PoÅ‚Ä…czenie percepcji z IK
- Testy na prawdziwym robocie
- Optymalizacja precyzji

#### NarzÄ™dzia matematyczne

**Kinematyka prosta (Forward Kinematics):**
```
Dany: kÄ…ty stawÃ³w [Î¸1, Î¸2, Î¸3, Î¸4, Î¸5]
ZnajdÅº: pozycja koÅ„cÃ³wki ramienia [x, y, z]
```

**Kinematyka odwrotna (Inverse Kinematics):**
```
Dany: docelowa pozycja [x, y, z]
ZnajdÅº: kÄ…ty stawÃ³w [Î¸1, Î¸2, Î¸3, Î¸4, Î¸5]
```

**Biblioteki pomocnicze:**
- **KDL (Kinematics and Dynamics Library)** - solwer IK
- **MoveIt2** - planowanie ruchu
- **tf2** - transformacje przestrzenne

#### PrzykÅ‚ad uÅ¼ycia tf2
```cpp
#include <tf2_ros/transform_listener.h>
#include <tf2_geometry_msgs/tf2_geometry_msgs.h>

// Transformacja punktu z ukÅ‚adu kamery do ukÅ‚adu bazowego robota
geometry_msgs::msg::PointStamped point_camera;
point_camera.header.frame_id = "camera_link";
point_camera.point.x = 0.5;  // Punkt wykryty przez kamerÄ™

geometry_msgs::msg::PointStamped point_base;
tf_buffer->transform(point_camera, point_base, "base_link");
// Teraz point_base zawiera wspÃ³Å‚rzÄ™dne w ukÅ‚adzie robota
```

#### Rozszerzenia
- ğŸ¤– Åšledzenie ruchomych obiektÃ³w
- ğŸ‘€ Koordynacja gÅ‚owy (jeÅ›li G1 ma ruchomÄ… gÅ‚owÄ™)
- ğŸ“ DotkniÄ™cie obiektu (reach and touch)
- ğŸ—£ï¸ Interakcja gÅ‚osowa "wskaÅ¼ stÃ³Å‚", "wskaÅ¼ drzwi"

---

### Projekt 5: Pick and Place ğŸ“¦

**Poziom:** ğŸŸ¡ Åšrednio-zaawansowany  
**Czas realizacji:** 4-5 tygodni  
**UmiejÄ™tnoÅ›ci:** Manipulacja, planowanie trajektorii, chwytak

#### Opis
PeÅ‚ny pipeline do podnoszenia i przenoszenia obiektÃ³w:
1. Wykryj obiekt
2. Zaplanuj trajektoriÄ™ do obiektu
3. Chwycenie (jeÅ›li G1 ma dÅ‚oÅ„/chwytak)
4. Przeniesienie do celu
5. Uwolnienie

#### Cele edukacyjne
- âœ… Planowanie trajektorii (path planning)
- âœ… Unikanie kolizji
- âœ… Sterowanie chwytakiem
- âœ… Koordynacja wielu systemÃ³w

#### State Machine
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IDLE   â”‚ â† Stan startowy
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ Wykryto obiekt
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DETECT â”‚ â† Identyfikacja i lokalizacja obiektu
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ Pozycja znana
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PLAN   â”‚ â† Planowanie trajektorii
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ Trajektoria gotowa
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REACH  â”‚ â† Ruch do obiektu
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ OsiÄ…gniÄ™to pozycjÄ™
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GRASP  â”‚ â† Chwycenie
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ Obiekt chwycony
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LIFT   â”‚ â† Podniesienie
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MOVE   â”‚ â† Przeniesienie do celu
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ OsiÄ…gniÄ™to cel
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RELEASE â”‚ â† Uwolnienie obiektu
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RETURN  â”‚ â† PowrÃ³t do pozycji startowej
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
     â””â”€â”€â†’ IDLE
```

#### Kamienie milowe
1. **TydzieÅ„ 1-2:** Detekcja obiektÃ³w i IK
2. **TydzieÅ„ 3:** Planowanie trajektorii i unikanie kolizji
3. **TydzieÅ„ 4:** Integracja z chwytakiem (jeÅ›li dostÄ™pny)
4. **TydzieÅ„ 5:** Testy end-to-end, optymalizacja

#### Rozszerzenia
- ğŸ¯ Sortowanie obiektÃ³w po kolorze/ksztaÅ‚cie
- ğŸ“ Precyzyjne ukÅ‚adanie (stacking)
- ğŸ”„ Wielokrotne podnoszenie (assembly)
- ğŸ§© RozwiÄ…zywanie prostych puzzli

---

### Projekt 6: Human Following ğŸ‘¤

**Poziom:** ğŸŸ¡ Åšrednio-zaawansowany  
**Czas realizacji:** 3-4 tygodnie  
**UmiejÄ™tnoÅ›ci:** Computer vision, nawigacja, tracking

#### Opis
Robot Å›ledzi osobÄ™ w przestrzeni:
- Wykrywa osobÄ™ (kamera + detekcja czÅ‚owieka)
- Orientuje siÄ™ w jej kierunku
- PodÄ…Å¼a za niÄ… w bezpiecznej odlegÅ‚oÅ›ci
- Zatrzymuje siÄ™ gdy osoba stoi

#### Cele edukacyjne
- âœ… Detekcja czÅ‚owieka (YOLO, HOG)
- âœ… Tracking (filtr Kalmana)
- âœ… Sterowanie lokomocjÄ…
- âœ… Regulacja prÄ™dkoÅ›ci

#### Architektura
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Camera  â”‚ â† RGB lub RGB-D
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚ Obraz
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Person  â”‚ â† CNN detector (YOLO, MobileNet-SSD)
â”‚ Detectionâ”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚ Bounding box
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Distance â”‚ â† Z depth camera lub lidar
â”‚  Est.    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚ OdlegÅ‚oÅ›Ä‡ i kÄ…t
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PID     â”‚ â† Regulacja: vx, vy, vyaw
â”‚Controllerâ”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚ PrÄ™dkoÅ›ci
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sport   â”‚ â† Sterowanie ruchem (jeÅ›li G1 moÅ¼e chodziÄ‡)
â”‚  Mode    â”‚   lub wizualne Å›ledzenie (obrÃ³t korpusu)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Strategia sterowania

**Regulator PID dla orientacji:**
```cpp
// BÅ‚Ä…d kÄ…towy (osoba w lewo/prawo od centrum)
double error_angle = person_x - image_center_x;

// Regulator proporcjonalny
double vyaw = Kp * error_angle;

// WyÅ›lij komendÄ™ ruchu
sport_client.Move(request, 0.0, 0.0, vyaw);
```

**Utrzymywanie odlegÅ‚oÅ›ci:**
```cpp
double desired_distance = 1.5;  // metry
double current_distance = depth_at_person;

double error_distance = current_distance - desired_distance;

// JeÅ›li za daleko - idÅº do przodu
// JeÅ›li za blisko - cofnij siÄ™
double vx = Kp_dist * error_distance;
```

#### Kamienie milowe
1. **TydzieÅ„ 1:** Detekcja osoby w obrazie
2. **TydzieÅ„ 2:** Estymacja odlegÅ‚oÅ›ci
3. **TydzieÅ„ 3:** Sterowanie orientacjÄ… (Å›ledzenie wzrokiem)
4. **TydzieÅ„ 4:** Lokomocja (jeÅ›li moÅ¼liwa) lub zaawansowane Å›ledzenie

#### Rozszerzenia
- ğŸ‘¥ Åšledzenie konkretnej osoby (re-identification)
- ğŸš§ Unikanie przeszkÃ³d podczas podÄ…Å¼ania
- ğŸ—£ï¸ Interakcje gÅ‚osowe podczas podÄ…Å¼ania
- ğŸ“¸ Rozpoznawanie gestÃ³w osoby

---

## Projekty zaawansowane

### Projekt 7: Autonomous Navigation ğŸ—ºï¸

**Poziom:** ğŸ”´ Zaawansowany  
**Czas realizacji:** 6-8 tygodni  
**UmiejÄ™tnoÅ›ci:** SLAM, path planning, lokalizacja

#### Opis
PeÅ‚na autonomiczna nawigacja w Å›rodowisku:
- SLAM (budowanie mapy otoczenia)
- Lokalizacja na mapie
- Planowanie Å›cieÅ¼ki
- Unikanie dynamicznych przeszkÃ³d

#### Technologie
- **Nav2** - ROS2 navigation stack
- **SLAM Toolbox** - budowanie map
- **AMCL** - lokalizacja
- **Costmap** - reprezentacja przeszkÃ³d

#### Rozszerzenia
- ğŸ¯ Wykrywanie i omijanie osÃ³b
- ğŸšª Otwieranie drzwi
- ğŸ›— Jazda windÄ…
- ğŸ“¡ Multi-robot coordination

---

### Projekt 8: Imitation Learning ğŸ“

**Poziom:** ğŸ”´ Zaawansowany  
**Czas realizacji:** 8-10 tygodni  
**UmiejÄ™tnoÅ›ci:** Machine learning, demonstracje, reinforcement learning

#### Opis
Robot uczy siÄ™ zachowaÅ„ poprzez obserwacjÄ™ demonstracji:
1. Nagrywanie trajektorii (kinesthetic teaching)
2. Uczenie modelu (behavioral cloning)
3. Odtwarzanie i generalizacja

#### Technologie
- **PyTorch/TensorFlow** - sieci neuronowe
- **ROS2 bag** - nagrywanie danych
- **DMPs (Dynamic Movement Primitives)** - reprezentacja trajektorii

#### PrzykÅ‚adowe zadania
- ğŸ¥¤ Nalewanie wody do szklanki
- ğŸ§¹ Zamiatanie
- ğŸ“ Pisanie

---

### Projekt 9: Dual-Arm Coordination ğŸ¤

**Poziom:** ğŸ”´ Zaawansowany  
**Czas realizacji:** 5-6 tygodni  
**UmiejÄ™tnoÅ›ci:** Koordynacja, constraints, planowanie

#### Opis
Zadania wymagajÄ…ce wspÃ³Å‚pracy obu ramion:
- Trzymanie dÅ‚ugich obiektÃ³w dwoma rÄ™kami
- Otwieranie wieczka (jedna rÄ™ka trzyma, druga otwiera)
- SkÅ‚adanie pudeÅ‚ka
- WiÄ…zanie supeÅ‚ka

#### Wyzwania
- Synchronizacja ruchÃ³w
- Ograniczenia (constraints) miÄ™dzy ramionami
- Planowanie w przestrzeni konfiguracyjnej 10-14D

---

## Projekty badawcze

### Projekt 10: Balance Control ğŸ¯

**Poziom:** ğŸ”´ Zaawansowany/Badawczy  
**Czas realizacji:** 10+ tygodni  
**Tematyka:** Sterowanie, teoria systemÃ³w, optymalizacja

#### Opis
Implementacja zaawansowanego algorytmu balansowania dla robota humanoidalnego:
- Model Predictive Control (MPC)
- Whole-body control
- Zero Moment Point (ZMP) planning
- Compensation for external disturbances

#### Tematy badawcze
- ğŸ“Š PorÃ³wnanie rÃ³Å¼nych algorytmÃ³w sterowania
- ğŸƒ Chodzenie po nierÃ³wnym terenie
- ğŸ¤¸ Reakcja na pchniÄ™cia
- âš–ï¸ Optymalizacja zuÅ¼ycia energii

---

## WskazÃ³wki realizacji

### Metodologia projektu

**1. Definicja wymagaÅ„**
- Co dokÅ‚adnie ma robiÄ‡ system?
- Jakie sÄ… kryteria sukcesu?
- Jakie sÄ… ograniczenia?

**2. Dekompozycja**
- Podziel projekt na moduÅ‚y
- Zdefiniuj interfejsy miÄ™dzy moduÅ‚ami
- Zaplanuj zaleÅ¼noÅ›ci

**3. Iteracyjny rozwÃ³j**
- Zacznij od najprostszej wersji (MVP)
- Testuj czÄ™sto
- Stopniowo dodawaj funkcjonalnoÅ›ci

**4. Testowanie**
- Testy jednostkowe (unit tests)
- Testy integracyjne
- Testy na prawdziwym sprzÄ™cie

**5. Dokumentacja**
- README z instrukcjami
- Komentarze w kodzie
- Diagramy architektury
- Filmy demonstracyjne

### Struktura repozytorium projektu

```
my_g1_project/
â”œâ”€â”€ README.md               â† Opis projektu
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md     â† Architektura systemu
â”‚   â”œâ”€â”€ setup.md           â† Instrukcje instalacji
â”‚   â””â”€â”€ results.md         â† Wyniki eksperymentÃ³w
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ perception/        â† ModuÅ‚ percepcji
â”‚   â”œâ”€â”€ planning/          â† ModuÅ‚ planowania
â”‚   â”œâ”€â”€ control/           â† ModuÅ‚ sterowania
â”‚   â””â”€â”€ utils/             â† NarzÄ™dzia pomocnicze
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ robot.yaml         â† Konfiguracja robota
â”‚   â””â”€â”€ parameters.yaml    â† Parametry algorytmÃ³w
â”œâ”€â”€ launch/
â”‚   â””â”€â”€ system.launch.py   â† Launch file ROS2
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ unit/              â† Testy jednostkowe
â”‚   â””â”€â”€ integration/       â† Testy integracyjne
â””â”€â”€ scripts/
    â”œâ”€â”€ run_demo.sh        â† Skrypt demonstracyjny
    â””â”€â”€ collect_data.py    â† Zbieranie danych
```

### Najlepsze praktyki

**Wersjonowanie (Git):**
```bash
# RozgaÅ‚Ä™zienia dla funkcjonalnoÅ›ci
git checkout -b feature/object-detection
git checkout -b feature/grasping
git checkout -b bugfix/ik-solver

# CzÄ™ste commity z opisami
git commit -m "Add: inverse kinematics solver for G1 arm"
git commit -m "Fix: joint limits checking"
```

**Testy:**
```bash
# Pisz testy dla krytycznych funkcji
colcon test --packages-select my_g1_project
```

**Dokumentacja:**
```cpp
/**
 * @brief Oblicza kinematykÄ™ odwrotnÄ… dla ramienia G1
 * 
 * @param target_pos Docelowa pozycja [x, y, z] w metrach
 * @param joint_angles [out] Wynikowe kÄ…ty stawÃ³w [rad]
 * @return true jeÅ›li rozwiÄ…zanie znalezione, false w przeciwnym razie
 * 
 * @note Funkcja uÅ¼ywa algorytmu Levenberg-Marquardt
 * @warning SprawdÅº limity stawÃ³w przed wysÅ‚aniem do robota
 */
bool inverse_kinematics(const Vector3d& target_pos, 
                       std::vector<double>& joint_angles);
```

### Zasoby pomocnicze

**Dokumentacja:**
- ROS2 Documentation: https://docs.ros.org/
- Unitree Support: https://support.unitree.com/
- MoveIt2: https://moveit.picknik.ai/

**Kursy online:**
- ROS2 Tutorials (oficjalne)
- Gazebo Simulation
- Computer Vision (OpenCV, PCL)

**SpoÅ‚ecznoÅ›ci:**
- ROS Discourse: https://discourse.ros.org/
- Robotics Stack Exchange
- GitHub Issues w unitree_ros2

---

## Podsumowanie

**WybÃ³r projektu:**
- ğŸŸ¢ **PoczÄ…tkujÄ…cy:** Projekty 1-3
- ğŸŸ¡ **Åšrednio-zaawansowani:** Projekty 4-6
- ğŸ”´ **Zaawansowani:** Projekty 7-10

**Kluczowe umiejÄ™tnoÅ›ci:**
- âœ… Programowanie w C++ i Python
- âœ… ZnajomoÅ›Ä‡ ROS2
- âœ… Podstawy kinematyki i dynamiki
- âœ… Podstawy computer vision (dla projektÃ³w z percepcjÄ…)
- âœ… Podstawy machine learning (dla projektÃ³w z ML)

**PamiÄ™taj:**
- ğŸ“š Czytaj dokumentacjÄ™
- ğŸ§ª Testuj czÄ™sto
- ğŸ’¾ Wersjonuj kod
- ğŸ¤ WspÃ³Å‚pracuj z innymi
- ğŸ¯ Zacznij od maÅ‚ych celÃ³w
- ğŸ” BezpieczeÅ„stwo przede wszystkim!

---

**Powodzenia w realizacji projektÃ³w! ğŸ¤–ğŸ“ğŸš€**
